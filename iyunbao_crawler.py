#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import json
from datetime import datetime
import mysql.connector
from mysql.connector import Error
import time
import logging
import argparse
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    'host': '172.105.225.120',
    'user': 'root',
    'password': 'lnmp.org#25295',
    'database': 'wordpress',
    'port': 3306
}

# APIåŸºç¡€é…ç½®
API_BASE_URL = 'https://api.iyunbao.com/discover/open/v1/post'
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

class IyunbaoCrawler:
    def __init__(self):
        self.db_connection = None
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
    
    def clean_html_content(self, html_content):
        """æ¸…ç†HTMLå†…å®¹ï¼Œç§»é™¤ä¸å¿…è¦å±æ€§ï¼Œç¡®ä¿å›¾ç‰‡èƒ½æ­£å¸¸æ˜¾ç¤º"""
        if not html_content:
            return html_content
        
        # 1. ç§»é™¤ _src å±æ€§ï¼ˆä¿ç•™ src å±æ€§ï¼‰
        html_content = re.sub(r'\s+_src="[^"]*"', '', html_content)
        
        # 2. ç§»é™¤å…¶ä»–å¯èƒ½å¯¼è‡´é—®é¢˜çš„å±æ€§
        # ç§»é™¤ style="" ä¸­çš„ç©ºå±æ€§
        html_content = re.sub(r'\s+style=""', '', html_content)
        
        # 3. æ¸…ç†å¤šä¸ªç©ºæ ¼
        html_content = re.sub(r'  +', ' ', html_content)
        
        # 4. ä¼˜åŒ–imgæ ‡ç­¾ - ç¡®ä¿imgæ ‡ç­¾æ ¼å¼æ­£ç¡®
        # ç§»é™¤imgæ ‡ç­¾ä¸­çš„å¤šä½™å±æ€§
        def fix_img_tag(match):
            img_tag = match.group(0)
            # ä¿ç•™ src å±æ€§ï¼Œç§»é™¤ _src
            img_tag = re.sub(r'\s+_src="[^"]*"', '', img_tag)
            return img_tag
        
        html_content = re.sub(r'<img[^>]*>', fix_img_tag, html_content)
        
        return html_content
        
    def connect_db(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.db_connection = mysql.connector.connect(**DB_CONFIG)
            logger.info("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")
            return True
        except Error as e:
            logger.error(f"âœ— æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
    
    def close_db(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.db_connection and self.db_connection.is_connected():
            self.db_connection.close()
            logger.info("âœ“ æ•°æ®åº“è¿æ¥å·²å…³é—­")
    
    def fetch_article(self, post_id):
        """è·å–å•ç¯‡æ–‡ç« ï¼ˆä½¿ç”¨APIï¼‰"""
        try:
            url = f"{API_BASE_URL}/{post_id}?_version=5.3.0&_client=2"
            logger.info(f"æ­£åœ¨è·å–æ–‡ç«  #{post_id}...")
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            if not data.get('isSuccess'):
                logger.warning(f"âœ— æ–‡ç«  #{post_id} è·å–å¤±è´¥: {data.get('errorMsg')}")
                return None
            
            result = data.get('result', {})
            
            # æå–æ•°æ®
            title = result.get('title', 'æ— æ ‡é¢˜')
            content_html = result.get('content', '<p>æ— å†…å®¹</p>')
            
            # æ¸…ç†HTMLå†…å®¹ - ç§»é™¤ä¸å¿…è¦çš„å±æ€§ï¼Œç¡®ä¿å›¾ç‰‡èƒ½æ­£å¸¸æ˜¾ç¤º
            content_html = self.clean_html_content(content_html)
            
            read_count = int(result.get('postPv', -1))
            like_count = int(result.get('likeNum', -1))
            author_name = result.get('author', {}).get('nickname', 'å¤´æ¡å¦¹å¦¹')
            
            article_data = {
                'src_url': f"https://bbs.iyunbao.com/m/community/topic?a=1&postId={post_id}",
                'src_title': title[:191],  # é™åˆ¶é•¿åº¦
                'src_content': content_html,  # å·²æ¸…ç†çš„HTML
                'read_count': read_count,
                'like_count': like_count,
                'src_user': author_name,
                'from_source': 'iyunbao',
                'create_time': datetime.now(),
                'post_id': post_id
            }
            
            logger.info(f"âœ“ æˆåŠŸè§£ææ–‡ç«  #{post_id}")
            logger.info(f"  æ ‡é¢˜: {title[:80]}")
            logger.info(f"  é˜…è¯»æ•°: {read_count}, çœ‹å¥½æ•°: {like_count}")
            
            return article_data
            
        except requests.RequestException as e:
            logger.error(f"âœ— ç½‘ç»œè¯·æ±‚å¤±è´¥ #{post_id}: {e}")
            return None
        except Exception as e:
            logger.error(f"âœ— è§£ææ–‡ç«  #{post_id} å¤±è´¥: {e}")
            return None
    
    def save_article_to_local(self, article_data):
        """ä¿å­˜ç¬¬ä¸€ç¯‡æ–‡ç« åˆ°æœ¬åœ°"""
        try:
            filename = f"first_article_{article_data['post_id']}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(article_data, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"âœ“ ç¬¬ä¸€ç¯‡æ–‡ç« å·²ä¿å­˜åˆ°: {filename}")
            return True
        except Exception as e:
            logger.error(f"âœ— ä¿å­˜æ–‡ç« åˆ°æœ¬åœ°å¤±è´¥: {e}")
            return False
    
    def check_article_exists(self, article_url):
        """æ£€æŸ¥æ–‡ç« URLæ˜¯å¦å·²å­˜åœ¨æ•°æ®åº“ä¸­"""
        try:
            cursor = self.db_connection.cursor()
            query = "SELECT id FROM baoxianblog WHERE src_url = %s LIMIT 1"
            cursor.execute(query, (article_url,))
            result = cursor.fetchone()
            cursor.close()
            return result is not None
        except Error as e:
            logger.warning(f"âš ï¸  æ£€æŸ¥URLé‡å¤æ—¶å‡ºé”™: {e}")
            return False
    
    def insert_article_to_db(self, article_data):
        """å°†æ–‡ç« æ’å…¥æ•°æ®åº“"""
        try:
            cursor = self.db_connection.cursor()
            
            query = """
            INSERT INTO baoxianblog 
            (src_url, src_title, src_content, read_count, like_count, src_user, 
             from_source, create_time, update_time, isPublish, published_user)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            values = (
                article_data['src_url'],
                article_data['src_title'],
                article_data['src_content'],
                article_data['read_count'],
                article_data['like_count'],
                article_data['src_user'],
                article_data['from_source'],
                article_data['create_time'],
                article_data['create_time'],
                0,  # isPublish
                article_data['src_user']
            )
            
            cursor.execute(query, values)
            self.db_connection.commit()
            
            logger.info(f"âœ“ æ–‡ç« å·²å†™å…¥æ•°æ®åº“: {article_data['src_title'][:60]}")
            cursor.close()
            return True
            
        except Error as e:
            logger.error(f"âœ— æ•°æ®åº“å†™å…¥å¤±è´¥: {e}")
            self.db_connection.rollback()
            return False
    
    def check_db_data(self):
        """æŸ¥çœ‹æ•°æ®åº“ä¸­å·²ä¿å­˜çš„æ–‡ç« """
        try:
            cursor = self.db_connection.cursor()
            query = """
            SELECT id, src_title, read_count, like_count, src_user 
            FROM baoxianblog 
            WHERE from_source='iyunbao' 
            ORDER BY id DESC 
            LIMIT 5
            """
            cursor.execute(query)
            results = cursor.fetchall()
            
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ“Š æ•°æ®åº“ä¸­çš„æ–‡ç« æ•°æ®ï¼ˆæœ€æ–°5æ¡ï¼‰")
            logger.info("=" * 80)
            for row in results:
                logger.info(f"  ID: {row[0]:6} | æ ‡é¢˜: {row[1][:50]:<50} | é˜…è¯»: {row[2]:<6} | çœ‹å¥½: {row[3]:<6} | ä½œè€…: {row[4]}")
            logger.info("=" * 80)
            
            cursor.close()
        except Error as e:
            logger.error(f"âœ— æŸ¥è¯¢æ•°æ®åº“å¤±è´¥: {e}")
    
    def crawl_articles(self, start_post_id=97867, count=3):
        """çˆ¬å–æŒ‡å®šæ•°é‡çš„æ–‡ç« """
        if not self.connect_db():
            logger.error("âœ— æ— æ³•è¿æ¥æ•°æ®åº“ï¼Œçˆ¬è™«é€€å‡º")
            return False
        
        try:
            current_post_id = start_post_id
            success_count = 0  # æ–°å¢æ–‡ç« æ•°
            skip_count = 0     # å·²å­˜åœ¨ï¼ˆè·³è¿‡ï¼‰æ•°
            fail_count = 0     # çœŸå®å¤±è´¥æ•°
            first_article_saved = False
            consecutive_fails = 0  # è¿ç»­å¤±è´¥æ¬¡æ•°
            max_consecutive_fails = 20  # è¿ç»­å¤±è´¥20æ¬¡æ‰åœæ­¢
            
            while success_count < count and consecutive_fails < max_consecutive_fails:
                logger.info(f"\n{'='*80}")
                logger.info(f"ğŸ“ æ­£åœ¨çˆ¬å–ç¬¬ {success_count + skip_count + 1}ä¸ª (postId: {current_post_id}, æˆåŠŸ: {success_count}/{count})")
                logger.info(f"{'='*80}")
                
                article_data = self.fetch_article(current_post_id)
                
                if article_data:
                    # æ£€æŸ¥æ–‡ç« URLæ˜¯å¦å·²å­˜åœ¨
                    if self.check_article_exists(article_data['src_url']):
                        logger.info(f"â­ï¸  æ–‡ç« å·²å­˜åœ¨æ•°æ®åº“ä¸­ï¼ˆè·³è¿‡ï¼‰: {article_data['src_title'][:60]}")
                        skip_count += 1
                        consecutive_fails = 0  # é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
                    else:
                        # ä¿å­˜ç¬¬ä¸€ç¯‡æ–°æ–‡ç« åˆ°æœ¬åœ°
                        if not first_article_saved:
                            self.save_article_to_local(article_data)
                            first_article_saved = True
                        
                        # æ’å…¥æ•°æ®åº“
                        if self.insert_article_to_db(article_data):
                            success_count += 1
                            consecutive_fails = 0  # é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
                            logger.info(f"âœ“ æˆåŠŸçˆ¬å– {success_count}/{count} ç¯‡æ–‡ç«  (æ–°å¢, å·²è·³è¿‡ {skip_count} ç¯‡)")
                        else:
                            logger.warning(f"âœ— æ’å…¥æ•°æ®åº“å¤±è´¥ï¼Œè·³è¿‡è¯¥æ–‡ç« ")
                            fail_count += 1
                            consecutive_fails += 1
                else:
                    logger.warning(f"âœ— è·å–æ–‡ç« å¤±è´¥ï¼Œè·³è¿‡è¯¥æ–‡ç« ")
                    fail_count += 1
                    consecutive_fails += 1
                
                # å¤„ç†ä¸‹ä¸€ç¯‡æ–‡ç« ï¼ˆpostIdä»å¤§åˆ°å°ï¼‰
                current_post_id -= 1
                
                # å»¶è¿Ÿè¯·æ±‚ï¼Œé¿å…è¢«åçˆ¬ (å»ºè®®2-3ç§’)
                if success_count < count:
                    time.sleep(3)  # å¢åŠ åˆ°3ç§’ä»¥é¿å…åçˆ¬
            
            # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
            logger.info(f"\n{'='*80}")
            logger.info(f"âœ“ çˆ¬è™«ä»»åŠ¡å®Œæˆç»Ÿè®¡")
            logger.info(f"{'='*80}")
            logger.info(f"  æ–°å¢æ–‡ç« : {success_count} ç¯‡")
            logger.info(f"  å·²å­˜åœ¨: {skip_count} ç¯‡")
            logger.info(f"  å¤±è´¥: {fail_count} ç¯‡")
            logger.info(f"  æ€»å¤„ç†: {success_count + skip_count + fail_count} ç¯‡")
            
            if success_count >= count:
                logger.info(f"âœ“ å·²æˆåŠŸçˆ¬å–ç›®æ ‡æ•°é‡ {count} ç¯‡æ–‡ç« ")
            elif consecutive_fails >= max_consecutive_fails:
                logger.warning(f"âš ï¸  è¿ç»­å¤±è´¥ {consecutive_fails} æ¬¡ï¼Œåœæ­¢çˆ¬è™«")
            else:
                logger.warning(f"âš ï¸  ä»…æˆåŠŸçˆ¬å– {success_count}/{count} ç¯‡æ–°æ–‡ç«  (è¿˜è·³è¿‡äº† {skip_count} ç¯‡å·²å­˜åœ¨æ–‡ç« )")
            
            logger.info(f"{'='*80}\n")
            
            # æ˜¾ç¤ºæ•°æ®åº“ä¸­çš„æ•°æ®
            self.check_db_data()
            
            return success_count >= count
            
        except Exception as e:
            logger.error(f"âœ— çˆ¬è™«æ‰§è¡Œå‡ºé”™: {e}")
            return False
        finally:
            self.close_db()


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='iäº‘ä¿çˆ¬è™« - æ‰¹é‡æŠ“å–iäº‘ä¿æ–‡ç« åˆ°æ•°æ®åº“',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ç¤ºä¾‹ï¼š
  python3 iyunbao_crawler.py                    # ä½¿ç”¨é»˜è®¤å‚æ•°ï¼ˆpostId: 97867, çˆ¬å–3ç¯‡ï¼‰
  python3 iyunbao_crawler.py --start 97867 --count 5   # ä»97867å¼€å§‹ï¼Œçˆ¬å–5ç¯‡
  python3 iyunbao_crawler.py -s 97800 -c 10    # ç®€å†™å½¢å¼
        '''
    )
    
    parser.add_argument(
        '--start', '-s',
        type=int,
        default=97867,
        help='èµ·å§‹æ–‡ç« IDï¼ˆpostIdï¼‰ï¼Œé»˜è®¤ï¼š97867'
    )
    
    parser.add_argument(
        '--count', '-c',
        type=int,
        default=3,
        help='è¦çˆ¬å–çš„æ–‡ç« æ•°é‡ï¼Œé»˜è®¤ï¼š3'
    )
    
    args = parser.parse_args()
    
    # å‚æ•°éªŒè¯
    if args.start < 1:
        logger.error("âœ— èµ·å§‹IDå¿…é¡»å¤§äº0")
        return False
    
    if args.count < 1:
        logger.error("âœ— çˆ¬å–æ•°é‡å¿…é¡»å¤§äº0")
        return False
    
    logger.info("\n" + "=" * 80)
    logger.info("ğŸš€ iäº‘ä¿çˆ¬è™«å¯åŠ¨")
    logger.info("=" * 80)
    logger.info(f"ğŸ“ å‚æ•°é…ç½®ï¼š")
    logger.info(f"   èµ·å§‹ID (postId)ï¼š{args.start}")
    logger.info(f"   çˆ¬å–æ•°é‡ï¼š{args.count}")
    logger.info("=" * 80 + "\n")
    
    crawler = IyunbaoCrawler()
    
    # æ ¹æ®å‚æ•°çˆ¬å–æ–‡ç« 
    success = crawler.crawl_articles(start_post_id=args.start, count=args.count)
    
    if success:
        logger.info(f"\nâœ“ ä»»åŠ¡å®Œæˆï¼å·²æˆåŠŸçˆ¬å– {args.count} ç¯‡æ–‡ç« å¹¶ä¿å­˜åˆ°æ•°æ®åº“ã€‚")
    else:
        logger.error(f"\nâœ— ä»»åŠ¡æœªèƒ½å…¨éƒ¨å®Œæˆï¼ˆç›®æ ‡ï¼š{args.count}ç¯‡ï¼‰ã€‚")
    
    return success


if __name__ == '__main__':
    main()
